{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aujourd'hui on roule sur les mecs de l'ENS\n",
    "\n",
    "\n",
    "https://challengedata.ens.fr/en/challenge/39/prediction_of_transaction_claims_status.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports des librairies de bases\n",
    "\n",
    "On ajoutera celles qui manquent au fur et à mesure de nos besoins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To support both python 2 and python 3\n",
    "from __future__ import division, print_function, unicode_literals\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os, gc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Définition de la seed pour le random\n",
    "\n",
    "Très important pour qu'on voit les mêmes choses entre nos deux ordis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 42;\n",
    "np.random.seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Définition des paramètres pour Matplot\n",
    "\n",
    "Rien de bien intéréssant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 12\n",
    "plt.rcParams['ytick.labelsize'] = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set des variables globales\n",
    "\n",
    "Attention, je n'utilise les variables globales pour la gestion des fichiers. Sinon, c'est mort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where to save the figures\n",
    "PROJECT_ROOT_DIR = \".\"\n",
    "DATA_PROCESSED = os.path.join(PROJECT_ROOT_DIR, \"data_processed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fonction pour load les libraires\n",
    "\n",
    "En vrai, on a juste besoin de pd.read_csv, mais c'était pour faire joli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file,data_path=DATA_PROCESSED, sep=';'):\n",
    "    csv_path = os.path.join(data_path, file)\n",
    "    return pd.read_csv(csv_path, sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# On load les jeux de données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "TX_data = load_data(file = \"train.csv\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "TX_data.drop(['CARD_PAYMENT','COUPON_PAYMENT','RSP_PAYMENT','WALLET_PAYMENT'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 100000 entries, 0 to 99999\n",
      "Data columns (total 57 columns):\n",
      "SHIPPING_MODE                 100000 non-null object\n",
      "SHIPPING_PRICE                100000 non-null int64\n",
      "WARRANTIES_FLG                100000 non-null bool\n",
      "WARRANTIES_PRICE              100000 non-null int64\n",
      "PRICECLUB_STATUS              100000 non-null int64\n",
      "REGISTRATION_DATE             100000 non-null int64\n",
      "PURCHASE_COUNT                100000 non-null int64\n",
      "BUYER_BIRTHDAY_DATE           100000 non-null float64\n",
      "BUYER_DEPARTMENT              100000 non-null int64\n",
      "BUYING_DATE                   100000 non-null int64\n",
      "SELLER_SCORE_COUNT            100000 non-null int64\n",
      "SELLER_SCORE_AVERAGE          100000 non-null float64\n",
      "SELLER_COUNTRY                100000 non-null object\n",
      "SELLER_DEPARTMENT             100000 non-null int64\n",
      "PRODUCT_TYPE                  100000 non-null object\n",
      "PRODUCT_FAMILY                100000 non-null object\n",
      "ITEM_PRICE                    100000 non-null int64\n",
      "CLAIM_TYPE                    100000 non-null object\n",
      "WARRANTY_COV_RATE             100000 non-null float64\n",
      "SELLER_COUNTRY_GINI           100000 non-null int64\n",
      "SELLER_COUNTRY_PIB            100000 non-null int64\n",
      "SELLER_COUNTRY_DISTANCE       100000 non-null int64\n",
      "BUYER_DEPARTMENT_DENSITY      100000 non-null int64\n",
      "SELLER_DEPARTMENT_DENSITY     100000 non-null int64\n",
      "BUYER_DEPARTMENT_PIB          100000 non-null int64\n",
      "SELLER_DEPARTMENT_PIB         100000 non-null int64\n",
      "BUYER_DEPARTMENT_Life_Lvl     100000 non-null int64\n",
      "SELLER_DEPARTMENT_Life_Lvl    100000 non-null int64\n",
      "BUYER_DEPARTMENT_interD       100000 non-null float64\n",
      "SELLER_DEPARTMENT_interD      100000 non-null float64\n",
      "BUYER_DEPARTMENT_Pov          100000 non-null float64\n",
      "SELLER_DEPARTMENT_Pov         100000 non-null float64\n",
      "BUYER_DEPARTMENT_min_soc      100000 non-null float64\n",
      "SELLER_DEPARTMENT_min_soc     100000 non-null float64\n",
      "BUYER_DEPARTMENT_atk          100000 non-null float64\n",
      "SELLER_DEPARTMENT_atk         100000 non-null float64\n",
      "BUYER_DEPARTMENT_cmb          100000 non-null float64\n",
      "SELLER_DEPARTMENT_cmb         100000 non-null float64\n",
      "CAC_POINTS                    100000 non-null int64\n",
      "CAC_VAR                       100000 non-null int64\n",
      "IS_HOLIDAYS                   100000 non-null int64\n",
      "IS_SALES                      100000 non-null int64\n",
      "Delivery_Quality              100000 non-null int64\n",
      "PGC_FAMILY                    100000 non-null int64\n",
      "PGC_TYPE                      100000 non-null int64\n",
      "Daily_Usage                   100000 non-null int64\n",
      "Consommable                   100000 non-null int64\n",
      "New_Techno                    100000 non-null int64\n",
      "Long_Term_Usage               100000 non-null int64\n",
      "Achat_Passion                 100000 non-null int64\n",
      "SELLER_ID                     100000 non-null object\n",
      "Count_Sells_ID                100000 non-null float64\n",
      "Mean_Claims_Sells_ID          100000 non-null float64\n",
      "STD_Claims_Sells_ID           100000 non-null float64\n",
      "Count_PRODUCT_TYPE            100000 non-null float64\n",
      "Mean_Claims_PRODUCT_TYPE      100000 non-null float64\n",
      "STD_Claims_PRODUCT_TYPE       100000 non-null float64\n",
      "dtypes: bool(1), float64(19), int64(31), object(6)\n",
      "memory usage: 42.8+ MB\n"
     ]
    }
   ],
   "source": [
    "TX_data.info() # 42 colonnes, c'est un nombre qui fait plaisir. [Note, il y en a plus maintenant, mais ce commentaire me fait toujours rire]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split entre train et test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_set, test_set = train_test_split(TX_data, \n",
    "                                       test_size=0.2, \n",
    "                                       random_state=RANDOM_SEED, \n",
    "                                       stratify=TX_data[\"CLAIM_TYPE\"]\n",
    "                                      )\n",
    "del TX_data;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jointure entre les X et Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def datapreprocess(data):\n",
    "    data=data.apply(pd.to_numeric, errors='ignore')\n",
    "    \n",
    "    # Y and X\n",
    "    try :\n",
    "        Y=data[\"CLAIM_TYPE\"]\n",
    "        X=data.drop(\"CLAIM_TYPE\", axis=1,inplace=False)\n",
    "    except:\n",
    "        Y=0\n",
    "        X=data\n",
    "    # Exclude Objets\n",
    "    X=X.select_dtypes(exclude=['object']) # j'exclude les variables catégorielles que j'ai oublié\n",
    "    \n",
    "    # Work on fare\n",
    "    from sklearn.preprocessing import Imputer\n",
    "    imp = Imputer(missing_values='NaN',strategy='median', axis=1)\n",
    "    X=pd.DataFrame(imp.fit_transform(X),columns=X.columns.values)\n",
    " \n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "559"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, Y_train = datapreprocess(train_set)\n",
    "X_test, Y_test = datapreprocess(test_set)\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiclass_roc_auc_score(truth, pred):\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    from sklearn.preprocessing import LabelBinarizer\n",
    "    lb = LabelBinarizer()\n",
    "    lb.fit(truth)\n",
    "    return roc_auc_score(lb.transform(truth), lb.transform(pred), average=\"weighted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "def plot_confusion_matrix(matrix):\n",
    "    \"\"\"If you prefer color and a colorbar\"\"\"\n",
    "    fig = plt.figure(figsize=(8,8))\n",
    "    ax = fig.add_subplot(111)\n",
    "    cax = ax.matshow(matrix)\n",
    "    fig.colorbar(cax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils.class_weight import compute_sample_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_weight_arr = compute_sample_weight(class_weight='balanced', y=Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Core XGBoost Library VS scikit-learn API\n",
    "\n",
    "Models can be trained in two different ways:\n",
    "\n",
    "1. Directly using the core library – this is closer to the implementation of the caret-package in R\n",
    "2. Using the scikit-learn API – this means that the models are implemented in a way that lets the scikit package recognize it as one of it’s own models.\n",
    "\n",
    "Nous, on va travailler avec l'API de Sklearn, c'est pas optimisé mais plus simple. De toute façon, j'arrive pas à utiliser le Core, a cause des DMatrix qui veulent que des numerics en entrées\n",
    "\n",
    "Doc des paramètres: https://github.com/dmlc/xgboost/blob/master/doc/parameter.md\n",
    "\n",
    "Doc sur le tunning : https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  XGBoost solo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_XGB={\n",
    "# General Parameters -  the overall functioning\n",
    "    'booster':'gbtree',\n",
    "    'silent':0,\n",
    "    #'nthread':4, # Je le commente, puisque il détecte automatiquement le nombre de cores qu'il peut utiliser.\n",
    "    'n_estimators' : 1000,\n",
    "    \n",
    "# Booster Parameters - the individual booster (tree/regression) at each step\n",
    "    'learning_rate' : 0.09,\n",
    "    'min_child_weight' : 1, #A smaller value is chosen because it is a highly imbalanced class problem and leaf nodes can have smaller size groups.\n",
    "    'max_depth' : 4,\n",
    "    #'max_leaf_nodes':None, #If this is defined, GBM will ignore max_depth.\n",
    "    'gamma' : 0.3,\n",
    "    'max_delta_step':10, #it might help in logistic regression when class is extremely imbalanced/ 1-10 might help control the update\n",
    "    'subsample' : 0.7,\n",
    "    'colsample_bytree' : 0.8,\n",
    "    'colsample_bylevel':1, #default\n",
    "    'reg_lambda' : 1, #default\n",
    "    'reg_alpha':0,\n",
    "    'scale_pos_weight' : sample_weight_arr,\n",
    "\n",
    "# Learning Task Parameters -  the optimization performed\n",
    "    'objective' : 'multi:softmax', # you also need to set an additional num_class (number of classes)\n",
    "    'num_class' : len(Y_train.unique()),\n",
    "    'eval_metric':\"auc\",\n",
    "    'seed' : RANDOM_SEED,\n",
    "    'random_state':RANDOM_SEED\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'base_score':0.5,\n",
    "'booster':'gbtree',\n",
    "'colsample_bylevel':1,\n",
    "'colsample_bytree':0.8,\n",
    "'eval_metric':'auc',\n",
    "'gamma':0.3,\n",
    "'learning_rate':0.09,\n",
    "'max_delta_step':1,\n",
    "'max_depth':2,\n",
    "'min_child_weight':1,\n",
    "'missing':None,\n",
    "'n_estimators':100,\n",
    "'n_jobs':1,\n",
    "'nthread':None,\n",
    "'num_class':8,\n",
    "'objective':'multi:softprob',\n",
    "'random_state':0,\n",
    "'reg_alpha':0,\n",
    "'reg_lambda':1,\n",
    "'seed':42,\n",
    "'silent':0,\n",
    "'subsample':0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_clf = XGBClassifier(**params_XGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_set = [(X_train, Y_train), (X_test, Y_test)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_XGB_fit={\n",
    "    'X':X_train, \n",
    "    'y':Y_train, \n",
    "    'sample_weight':sample_weight_arr, \n",
    "    #'eval_set':eval_set, \n",
    "    'eval_metric':'auc', \n",
    "    #'early_stopping_rounds':30, \n",
    "    'verbose':False, \n",
    "    'xgb_model':None\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=0.8, eval_metric='auc', gamma=0.3,\n",
       "       learning_rate=0.09, max_delta_step=10, max_depth=4,\n",
       "       min_child_weight=1, missing=None, n_estimators=1000, n_jobs=1,\n",
       "       nthread=None, num_class=8, objective='multi:softprob',\n",
       "       random_state=142, reg_alpha=0, reg_lambda=1,\n",
       "       scale_pos_weight=array([ 0.25012,  0.25012, ...,  3.07409,  0.25012]),\n",
       "       seed=142, silent=0, subsample=0.7)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_clf.fit(**params_XGB_fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_xgb_train = xgb_clf.predict(X_train)\n",
    "y_pred_xgb = xgb_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mAUC = multiclass_roc_auc_score(Y_train, y_pred_xgb_train)\n",
    "test_mAUC = multiclass_roc_auc_score(Y_test, y_pred_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance sur le train : 0.7063127833304117\n",
      "Performance sur le test : 0.6291750627623868\n"
     ]
    }
   ],
   "source": [
    "print(\"Performance sur le train : {}\".format(train_mAUC))\n",
    "print(\"Performance sur le test : {}\".format(test_mAUC))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performance sur le train : 0.7066348172305776\n",
    "\n",
    "Performance sur le test : 0.6256314390125582"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyse d'overfiting possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = xgb_clf.evals_result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = len(results['validation_0']['mlogloss'])\n",
    "x_axis = range(0, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot\n",
    "# plot log loss\n",
    "fig, ax = pyplot.subplots()\n",
    "ax.plot(x_axis, results['validation_0']['mlogloss'], label='Train')\n",
    "ax.plot(x_axis, results['validation_1']['mlogloss'], label='Test')\n",
    "ax.legend()\n",
    "pyplot.ylabel('AUC')\n",
    "pyplot.title('XGBoost AUC')\n",
    "pyplot.show()\n",
    "# plot classification error\n",
    "fig, ax = pyplot.subplots()\n",
    "ax.plot(x_axis, results['validation_0']['merror'], label='Train')\n",
    "ax.plot(x_axis, results['validation_1']['merror'], label='Test')\n",
    "ax.legend()\n",
    "pyplot.ylabel('Classification Error')\n",
    "pyplot.title('XGBoost Classification Error')\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_mx = confusion_matrix(Y_test, y_pred_xgb)\n",
    "row_sums = conf_mx.sum(axis=1, keepdims=True)\n",
    "norm_conf_mx = conf_mx / row_sums\n",
    "plot_confusion_matrix(norm_conf_mx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### C'est un beau score pour le XBoost\n",
    "\n",
    "Cependant, j'ai optimisé pour la mauvaise métrique, et j'ai toujours pas fait le Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(xgb_clf.feature_importances_, index=X_train.columns, columns=[\"Feature\"]).sort_values(by=\"Feature\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sortie en erreur les copines!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost Grid Search Iterate\n",
    "\n",
    "Comme mon PC est pourri, je vais chercher les paramètres de façon iterative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Paramètres pour le XGB qui ne changent pas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_XGB={\n",
    "# General Parameters -  the overall functioning\n",
    "    'booster':'gbtree',\n",
    "    'silent':0,\n",
    "    #'nthread':4, # Je vais le commenter, puisque il détecte automatiquement le nombre de cores qu'il peut utiliser.\n",
    "    'n_estimators' : 100,\n",
    "    \n",
    "# Booster Parameters - the individual booster (tree/regression) at each step\n",
    "    'learning_rate' : 0.09,\n",
    "    'colsample_bylevel':1, #default\n",
    "    'reg_lambda' : 1, #default\n",
    "    'scale_pos_weight' : sample_weight_arr,\n",
    "\n",
    "# Learning Task Parameters -  the optimization performed\n",
    "    'objective' : 'multi:softmax',\n",
    "    'num_class' : len(Y_train.unique()),\n",
    "    'eval_metric':\"auc\",\n",
    "    'seed' : RANDOM_SEED,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Paramètres pour la méthode `fit` de XGB qui ne changent pas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_params_xgb_cv={\n",
    "    'sample_weight': sample_weight_arr, \n",
    "    'eval_set' : None, \n",
    "    'eval_metric' : 'auc', \n",
    "    'early_stopping_rounds' : None, \n",
    "    'verbose':False, \n",
    "    'xgb_model':None\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Optim 1 : max_depth, min_child_weight et max_delta_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paramètres pour le GridSearch\n",
    "params_XGB_CV = {\n",
    "    'max_depth':range(2,5),\n",
    "    'min_child_weight':range(1,6),\n",
    "    'max_delta_step':list(range(1,5)),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_gs_cv = GridSearchCV(XGBClassifier(**params_XGB), \n",
    "                              params_XGB_CV,\n",
    "                              n_jobs=-1,\n",
    "                              verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_gs_cv.fit(\n",
    "    X = X_train, \n",
    "    y=Y_train, \n",
    "    groups=None, \n",
    "    **fit_params_xgb_cv\n",
    ")\n",
    "print(xgb_gs_cv.best_estimator_)\n",
    "print(\"ROC score : {}\".format(multiclass_roc_auc_score(Y_test, xgb_gs_cv.predict(X_test))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Donc les paramètres optimaux sont:\n",
    "1. `max_depth` : `2`\n",
    "2. `min_child_weight` : `1`\n",
    "3. `max_delta_step` : `1`\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Optim 2 : gamma, et subsample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_XGB={\n",
    "# General Parameters -  the overall functioning\n",
    "    'booster':'gbtree',\n",
    "    'silent':0,\n",
    "    #'nthread':4, # Je vais le commenter, puisque il détecte automatiquement le nombre de cores qu'il peut utiliser.\n",
    "    'n_estimators' : 100,\n",
    "    'max_depth' : 2,\n",
    "    'min_child_weight' : 1,\n",
    "    'max_delta_step' : 1,\n",
    "    \n",
    "# Booster Parameters - the individual booster (tree/regression) at each step\n",
    "    'learning_rate' : 0.09,\n",
    "    'colsample_bylevel':1, #default\n",
    "    'reg_lambda' : 1, #default\n",
    "    'scale_pos_weight' : sample_weight_arr,\n",
    "\n",
    "# Learning Task Parameters -  the optimization performed\n",
    "    'objective' : 'multi:softmax',\n",
    "    'num_class' : len(Y_train.unique()),\n",
    "    'eval_metric':\"auc\",\n",
    "    'seed' : RANDOM_SEED,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paramètres pour le GridSearch\n",
    "params_XGB_CV = {\n",
    "    'gamma':[i/10.0 for i in range(0,5)],\n",
    "    'subsample':[i/10.0 for i in range(6,10)],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_gs_cv = GridSearchCV(XGBClassifier(**params_XGB), \n",
    "                              params_XGB_CV,\n",
    "                              n_jobs=-1,\n",
    "                              verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_gs_cv.fit(\n",
    "    X = X_train, \n",
    "    y=Y_train, \n",
    "    groups=None, \n",
    "    **fit_params_xgb_cv\n",
    ")\n",
    "print(xgb_gs_cv.best_estimator_)\n",
    "print(\"ROC score : {}\".format(multiclass_roc_auc_score(Y_test, xgb_gs_cv.predict(X_test))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Donc les paramètres optimaux sont:\n",
    "1. `gamma` : `0.3`\n",
    "2. `subsample` : `0.7`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Optim 3 : colsample_bytree, et reg_alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_XGB={\n",
    "# General Parameters -  the overall functioning\n",
    "    'booster':'gbtree',\n",
    "    'silent':0,\n",
    "    #'nthread':4, # Je vais le commenter, puisque il détecte automatiquement le nombre de cores qu'il peut utiliser.\n",
    "    'n_estimators' : 100,\n",
    "    'max_depth' : 2,\n",
    "    'min_child_weight' : 1,\n",
    "    'max_delta_step' : 1,\n",
    "    'subsample' : 0.7,\n",
    "    \n",
    "# Booster Parameters - the individual booster (tree/regression) at each step\n",
    "    'learning_rate' : 0.09,\n",
    "    'colsample_bylevel':1, #default\n",
    "    'reg_lambda' : 1, #default\n",
    "    'scale_pos_weight' : sample_weight_arr,\n",
    "    'gamma' : 0.3,\n",
    "\n",
    "# Learning Task Parameters -  the optimization performed\n",
    "    'objective' : 'multi:softmax',\n",
    "    'num_class' : len(Y_train.unique()),\n",
    "    'eval_metric':\"auc\",\n",
    "    'seed' : RANDOM_SEED,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paramètres pour le GridSearch\n",
    "params_XGB_CV = {\n",
    "    'colsample_bytree':[i/10.0 for i in range(6,10)],\n",
    "    'reg_alpha':[0, 0.001, 0.005, 0.01, 0.05]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_gs_cv = GridSearchCV(XGBClassifier(**params_XGB), \n",
    "                              params_XGB_CV,\n",
    "                              n_jobs=-1,\n",
    "                              verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_gs_cv.fit(\n",
    "    X = X_train, \n",
    "    y=Y_train, \n",
    "    groups=None, \n",
    "    **fit_params_xgb_cv\n",
    ")\n",
    "print(xgb_gs_cv.best_estimator_)\n",
    "print(\"ROC score : {}\".format(multiclass_roc_auc_score(Y_test, xgb_gs_cv.predict(X_test))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Donc les paramètres optimaux sont:\n",
    "1. `colsample_bytree` : `0.8`\n",
    "2. `reg_alpha` : `0`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Optim 4 : L'ensemble des paramètres trouvés\n",
    "\n",
    "L'idée ici est de vérifier si les paramètres, ensemble, fonctionnent bien, donc on prend les paramètres trouvés, et on cherche un peu à gauche et à droite de chacun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_XGB={\n",
    "    'base_score':0.5,\n",
    "    'booster':'gbtree',\n",
    "    'eval_metric':'auc',\n",
    "    'learning_rate':0.09,\n",
    "    'missing':None,\n",
    "    'n_estimators':100,\n",
    "    'n_jobs':1,\n",
    "    'nthread':None,\n",
    "    'num_class':8,\n",
    "    'objective':'multi:softprob',\n",
    "    'random_state':RANDOM_SEED,\n",
    "    'reg_lambda':1,\n",
    "    'seed':RANDOM_SEED,\n",
    "    'silent':0,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_XGB_CV={\n",
    "    'colsample_bylevel':[1,2], #\n",
    "    'colsample_bytree':[0.7,0.8,0.9], #\n",
    "    'gamma':[0.2,0.3,0.4], #\n",
    "    'max_delta_step':[1,2], #\n",
    "    'max_depth':[2,3], #\n",
    "    'min_child_weight':[1,2], #\n",
    "    'reg_alpha':[0,0.1], #\n",
    "    'subsample':[0.6,0.7,0.8] #\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_gs_cv = GridSearchCV(XGBClassifier(**params_XGB), \n",
    "                              params_XGB_CV,\n",
    "                              n_jobs=-1,\n",
    "                              verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_gs_cv.fit(\n",
    "    X = X_train, \n",
    "    y=Y_train, \n",
    "    groups=None, \n",
    "    **fit_params_xgb_cv\n",
    ")\n",
    "print(xgb_gs_cv.best_estimator_)\n",
    "print(\"ROC score : {}\".format(multiclass_roc_auc_score(Y_test, xgb_gs_cv.predict(X_test))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_mx = confusion_matrix(Y_test, y_pred_xgb_cv)\n",
    "row_sums = conf_mx.sum(axis=1, keepdims=True)\n",
    "norm_conf_mx = conf_mx / row_sums\n",
    "plot_confusion_matrix(norm_conf_mx)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
