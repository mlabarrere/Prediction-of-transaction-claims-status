{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aujourd'hui on se fait rouler par les mecs de l'ENS\n",
    "\n",
    "\n",
    "https://challengedata.ens.fr/en/challenge/39/prediction_of_transaction_claims_status.html\n",
    "\n",
    "\n",
    "Ici, c'est le notebook dédié uniquement à:\n",
    "1. Gestion des données manquantes\n",
    "2. Correction des erreurs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "ea25cdf7-bdbc-3cf1-0737-bc51675e3374",
    "_uuid": "fed5696c67bf55a553d6d04313a77e8c617cad99",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "\n",
    "#load packages\n",
    "import sys #access to system parameters https://docs.python.org/3/library/sys.html\n",
    "print(\"Python version: {}\". format(sys.version))\n",
    "\n",
    "import pandas as pd #collection of functions for data processing and analysis modeled after R dataframes with SQL like features\n",
    "print(\"pandas version: {}\". format(pd.__version__))\n",
    "\n",
    "import matplotlib #collection of functions for scientific and publication-ready visualization\n",
    "print(\"matplotlib version: {}\". format(matplotlib.__version__))\n",
    "\n",
    "import numpy as np #foundational package for scientific computing\n",
    "print(\"NumPy version: {}\". format(np.__version__))\n",
    "\n",
    "import scipy as sp #collection of functions for scientific computing and advance mathematics\n",
    "print(\"SciPy version: {}\". format(sp.__version__)) \n",
    "\n",
    "import IPython\n",
    "from IPython import display #pretty printing of dataframes in Jupyter notebook\n",
    "print(\"IPython version: {}\". format(IPython.__version__)) \n",
    "\n",
    "import sklearn #collection of machine learning algorithms\n",
    "print(\"scikit-learn version: {}\". format(sklearn.__version__))\n",
    "\n",
    "import seaborn as sns\n",
    "print(\"seaborn version: {}\". format(sns.__version__))\n",
    "\n",
    "#misc libraries\n",
    "import time\n",
    "import random as rnd\n",
    "import os, gc\n",
    "\n",
    "#ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "print('-'*25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moi j'ai ça:\n",
    "\n",
    "Python version: 3.6.3 |Anaconda custom (64-bit)| (default, Oct 15 2017, 03:27:45) [MSC v.1900 64 bit (AMD64)]\n",
    "\n",
    "pandas version: 0.22.0\n",
    "\n",
    "matplotlib version: 2.1.2\n",
    "\n",
    "NumPy version: 1.12.1\n",
    "\n",
    "SciPy version: 1.0.0\n",
    "\n",
    "IPython version: 6.2.1\n",
    "\n",
    "scikit-learn version: 0.19.1\n",
    "\n",
    "seaborn version: 0.8.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run \"! pip install jyquickhelper\" dans une cellule si ca ne marche pas la commande suivante\n",
    "from jyquickhelper import add_notebook_menu\n",
    "add_notebook_menu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 12\n",
    "plt.rcParams['ytick.labelsize'] = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Définition de la seed pour le random\n",
    "\n",
    "Très important pour qu'on voit les mêmes choses entre nos deux ordis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 42;\n",
    "np.random.seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "6b5dc743-15b1-aac6-405e-081def6ecca1",
    "_uuid": "2d307b99ee3d19da3c1cddf509ed179c21dec94a"
   },
   "source": [
    "## Chargement des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where to save the figures\n",
    "PROJECT_ROOT_DIR = \".\"\n",
    "DATA_PATH = os.path.join(PROJECT_ROOT_DIR, \"data\")\n",
    "DATA_PROCESSED = os.path.join(PROJECT_ROOT_DIR, \"data_processed\")\n",
    "\n",
    "def load_data(file,data_path=DATA_PATH, sep=','):\n",
    "    csv_path = os.path.join(data_path, file)\n",
    "    return pd.read_csv(csv_path, sep)\n",
    "\n",
    "TX_data = load_data(file = \"input_train.csv\");\n",
    "STA_data = load_data(file = \"challenge_output_data_training_file_prediction_of_transaction_claims_status.csv\", sep=';');\n",
    "TX_data = pd.merge(TX_data, STA_data, left_index=True, right_index=True) # Merge de TX_data et STA_data en utilisant les index comme cléf primaire\n",
    "\n",
    "\n",
    "train_df=TX_data.drop([\"ID_y\",\"ID_x\"],inplace=False,axis=1) # On drop les ID qui sont crée dans le processus\n",
    "\n",
    "test_df = load_data(file = \"input_test.csv\");\n",
    "\n",
    "del STA_data #Je supprime toujours les variables inutiles pour liberer ma RAM (qui est très faible)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "3d6188f3-dc82-8ae6-dabd-83e28fcbf10d",
    "_uuid": "79282222056237a52bbbb1dbd831f057f1c23d69"
   },
   "source": [
    "## Analyse des types de données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "ce473d29-8d19-76b8-24a4-48c217286e42",
    "_uuid": "ef106f38a00e162a80c523778af6dcc778ccc1c2"
   },
   "outputs": [],
   "source": [
    "#train_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "cd19a6f6-347f-be19-607b-dca950590b37",
    "_uuid": "1d7acf42af29a63bc038f14eded24e8b8146f541"
   },
   "source": [
    "A l'état initial nous avons comme type de variables\n",
    "\n",
    "**Variables Catégorielles**\n",
    "1. `SHIPPING_MODE`\n",
    "2. `SHIPPING_PRICE`\n",
    "3. `WARRANTIES_PRICE`\n",
    "4. `PRICECLUB_STATUS`\n",
    "5. `PURCHASE_COUNT`\n",
    "7. `BUYING_DATE`\n",
    "8. `SELLER_SCORE_COUNT`\n",
    "9. `SELLER_SCORE_AVERAGE`\n",
    "10. `SELLER_COUNTRY`\n",
    "11. `PRODUCT_TYPE`\n",
    "12. `PRODUCT_FAMILY`\n",
    "13. `ITEM_PRICE`\n",
    "14. `CLAIM_TYPE` - La target\n",
    "\n",
    "**Variables Booléennes**\n",
    "1. `WARRANTIES_FLG`\n",
    "2. `CARD_PAYMENT`\n",
    "3. `COUPON_PAYMENT`\n",
    "4. `RSP_PAYMENT`\n",
    "5. `WALLET_PAYMENT`\n",
    "\n",
    "**Variables Numériques Discrètes**\n",
    "1. `REGISTRATION_DATE`\n",
    "2. `BUYER_BIRTHDAY_DATE`\n",
    "3. `BUYER_DEPARTMENT` - Que je propose de passer maitenent en catégorie, puisque les nombres n'ont pas de valeur + ou -\n",
    "4. `SELLER_DEPARTMENT` - Que je propose de passer maitenent en catégorie, puisque les nombres n'ont pas de valeur + ou -\n",
    "\n",
    "**Variables Numériques Continues**\n",
    " - Aucune\n",
    "\n",
    "**Variables Ordinales**\n",
    " - Aucune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.BUYER_DEPARTMENT = train_df.BUYER_DEPARTMENT.astype('object')\n",
    "train_df.SELLER_DEPARTMENT = train_df.SELLER_DEPARTMENT.astype('object')\n",
    "\n",
    "test_df.BUYER_DEPARTMENT = test_df.BUYER_DEPARTMENT.astype('object')\n",
    "test_df.SELLER_DEPARTMENT = test_df.SELLER_DEPARTMENT.astype('object')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "97f4e6f8-2fea-46c4-e4e8-b69062ee3d46",
    "_uuid": "c34fa51a38336d97d5f6a184908cca37daebd584"
   },
   "source": [
    "**Variables Mixtes**\n",
    "\n",
    "C'est à dire qu'elles contiennes en fait de l'alphanumérique, et des structures à l'interieur\n",
    "\n",
    "1. `SHIPPING_PRICE`         - Format Nombre </> Nombre\n",
    "2. `WARRANTIES_PRICE`         - Format Nombre </> Nombre\n",
    "3. `PURCHASE_COUNT`         - Format Nombre </> Nombre\n",
    "4. `BUYING_DATE`         - Format Nombre/2017\n",
    "5. `SELLER_SCORE_COUNT`         - Format Nombre </> Nombre\n",
    "6. `ITEM_PRICE`         - Format Nombre </> Nombre"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Données Manquantes**\n",
    "\n",
    "1. `SHIPPING_MODE`           : 315\n",
    "2. `SHIPPING_PRICE`          : 67610\n",
    "3. `WARRANTIES_PRICE`        : 96603\n",
    "4. `PRICECLUB_STATUS`        : 57\n",
    "5. `BUYER_BIRTHDAY_DATE`     : 5836\n",
    "6. `SELLER_SCORE_COUNT`      : 6\n",
    "7. `SELLER_SCORE_AVERAGE`    : 6\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Données Avec Erreurs**\n",
    "\n",
    "1. `BUYER_BIRTHDAY_DATE` - Des gens centenaires\n",
    "2. `BUYER_DEPARTMENT` - Départements -1, 0, 97, et 98 \n",
    "3. `SELLER_DEPARTMENT` - Départements avec -1\n",
    "4. `PRODUCT_FAMILY` - des \"White\" qui ne veulent rien dire\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "8bfe9610-689a-29b2-26ee-f67cd4719079",
    "_uuid": "699c52b7a8d076ccd5ea5bc5d606313c558a6e8e"
   },
   "source": [
    "**Dans l'ensemble**\n",
    "\n",
    "- 16 variables catégorielles dont 6 mixtes\n",
    "- 5 variables booléennes\n",
    "- 2 numériques\n",
    "\n",
    "- 8 ont des valeurs manquantes\n",
    "- 4 ont des erreurs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "9b805f69-665a-2b2e-f31d-50d87d52865d",
    "_uuid": "817e1cf0ca1cb96c7a28bb81192d92261a8bf427"
   },
   "outputs": [],
   "source": [
    "train_df.info()\n",
    "print('_'*40)\n",
    "test_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traiter les données manquantes\n",
    "\n",
    "Valeurs à remplir :\n",
    "1. `SHIPPING_MODE`  \n",
    "2. `SHIPPING_PRICE` \n",
    "3. `WARRANTIES_PRICE` \n",
    "4. `PRICECLUB_STATUS`\n",
    "5. `BUYER_BIRTHDAY_DATE` - Possède des erreurs, on va la traiter après\n",
    "6. `SELLER_SCORE_COUNT`\n",
    "7. `SELLER_SCORE_AVERAGE`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SHIPPING_MODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.SHIPPING_MODE.sort_values().unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## SHIPPING_MODE\n",
    "'''\n",
    "Propose de remplacer les valeurs manquantes par \"ABSENT\". Je souhaite faire comprendre à l'algorithme qu'il n'y a pas l'info\n",
    "\n",
    "Sinon, on peut remplacer par \"NORMAL\"\n",
    "'''\n",
    "\n",
    "train_df.SHIPPING_MODE.fillna(value=\"ABSENT\", inplace=True)\n",
    "test_df.SHIPPING_MODE.fillna(value=\"ABSENT\", inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SHIPPING_PRICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.SHIPPING_PRICE.sort_values().unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## SHIPPING_PRICE\n",
    "'''\n",
    "Propose de remplacer les valeurs manquantes par \"0\". Puisque c'était gratuit\n",
    "\n",
    "Sinon, on peut remplacer par \"<1\"\n",
    "'''\n",
    "\n",
    "train_df.SHIPPING_PRICE.fillna(value=\"0\", inplace=True)\n",
    "test_df.SHIPPING_PRICE.fillna(value=\"0\", inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WARRANTIES_PRICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.WARRANTIES_PRICE.sort_values().unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## WARRANTIES_PRICE \n",
    "'''\n",
    "Propose de remplacer les valeurs manquantes par \"0\". Puisque c'était gratuit\n",
    "\n",
    "Sinon, on peut remplacer par \"<1\"\n",
    "'''\n",
    "\n",
    "train_df.WARRANTIES_PRICE.fillna(value=\"0\", inplace=True)\n",
    "test_df.WARRANTIES_PRICE.fillna(value=\"0\", inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PRICECLUB_STATUS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_df.PRICECLUB_STATUS.sort_values().unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## PRICECLUB_STATUS \n",
    "'''\n",
    "Propose de remplacer les valeurs manquantes par \"UNSUBSCRIBED\". Puisque c'est par défaut\n",
    "\n",
    "Sinon, on peut remplacer par \"NONE\" pour montrer qu'il manque l'info\n",
    "'''\n",
    "\n",
    "train_df.PRICECLUB_STATUS.fillna(value=\"UNSUBSCRIBED\", inplace=True)\n",
    "test_df.PRICECLUB_STATUS.fillna(value=\"UNSUBSCRIBED\", inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SELLER_SCORE_COUNT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.SELLER_SCORE_COUNT.sort_values().unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## SELLER_SCORE_COUNT \n",
    "'''\n",
    "Propose de remplacer les valeurs manquantes par le mode de la série ('10000<100000').\n",
    "\n",
    "Sinon, on peut remplacer par \"-1\" pour montrer qu'il manque l'info\n",
    "'''\n",
    "\n",
    "mode_train= train_df.SELLER_SCORE_COUNT.mode()[0]\n",
    "\n",
    "\n",
    "train_df.SELLER_SCORE_COUNT.fillna(value=mode_train, inplace=True)\n",
    "test_df.SELLER_SCORE_COUNT.fillna(value=mode_train, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SELLER_SCORE_AVERAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_df.SELLER_SCORE_AVERAGE.sort_values().unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## SELLER_SCORE_AVERAGE \n",
    "'''\n",
    "Propose de remplacer les valeurs manquantes par la médiane de la série (46).\n",
    "\n",
    "Sinon, on peut remplacer par \"-1\" pour montrer qu'il manque l'info\n",
    "'''\n",
    "\n",
    "mode_train= train_df.SELLER_SCORE_AVERAGE.median()\n",
    "\n",
    "\n",
    "train_df.SELLER_SCORE_AVERAGE.fillna(value=mode_train, inplace=True)\n",
    "test_df.SELLER_SCORE_AVERAGE.fillna(value=mode_train, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traiter les erreurs\n",
    "\n",
    "Valeurs à corriger :\n",
    "\n",
    "1. `BUYER_BIRTHDAY_DATE` - Des gens centenaires\n",
    "2. `BUYER_DEPARTMENT` - Départements avec -1 et 0\n",
    "3. `SELLER_DEPARTMENT` - Départements avec -1\n",
    "4. `PRODUCT_FAMILY` - des \"White\" qui ne veulent rien dire"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BUYER_BIRTHDAY_DATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Avant\n",
    "sns.distplot(train_df.BUYER_BIRTHDAY_DATE.dropna(inplace=False));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu, sigma = train_df.BUYER_BIRTHDAY_DATE.mean(), train_df.BUYER_BIRTHDAY_DATE.std() # mean et standard deviation\n",
    "print(\"Mu = {} et sigma = {}\".format(mu, sigma))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normal_filler(value, mu, sigma, threshold):\n",
    "    if np.isnan(value) or value<threshold:\n",
    "        return np.random.normal(mu, sigma, 1)[0]\n",
    "    else:\n",
    "        return value;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.BUYER_BIRTHDAY_DATE = train_df.BUYER_BIRTHDAY_DATE.apply(normal_filler, args=(mu,sigma,1930))\n",
    "test_df.BUYER_BIRTHDAY_DATE = test_df.BUYER_BIRTHDAY_DATE.apply(normal_filler, args=(mu,sigma,1930))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Après\n",
    "sns.distplot(train_df.BUYER_BIRTHDAY_DATE);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BUYER_DEPARTMENT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On doit remplacer les valeurs:\n",
    "    -1, 0, 96, 97, 98 qui n'existent pas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"On a:\\n{} valeurs à -1\\n{} valeurs à 0\\n{} valeurs à 96\\n{} valeurs à 97\\n{} valeurs à 98\\n \".format(\n",
    "train_df[train_df.BUYER_DEPARTMENT == -1].shape[0],\n",
    "train_df[train_df.BUYER_DEPARTMENT == 0].shape[0],\n",
    "train_df[train_df.BUYER_DEPARTMENT == 96].shape[0],\n",
    "train_df[train_df.BUYER_DEPARTMENT == 97].shape[0],\n",
    "train_df[train_df.BUYER_DEPARTMENT == 98].shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Comme ca n'a pas de sens de remplir avec une loi normale\n",
    "je propose de corriger tout ça en mettant tout à \"0\" pour montrer à l'algorithme le problème\n",
    "''' \n",
    "\n",
    "def departement_filler(value):\n",
    "    if value not in range(-1,96):\n",
    "        return 0;\n",
    "    else:\n",
    "        return value;\n",
    "\n",
    "train_df.BUYER_DEPARTMENT = train_df.BUYER_DEPARTMENT.apply(departement_filler)\n",
    "test_df.BUYER_DEPARTMENT = test_df.BUYER_DEPARTMENT.apply(departement_filler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SELLER_DEPARTMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"On a:\\n{} valeurs à -1\\n{} valeurs à 0\\n{} valeurs à 96\\n{} valeurs à 97\\n{} valeurs à 98\\n \".format(\n",
    "            train_df[train_df.SELLER_DEPARTMENT == -1].shape[0],\n",
    "            train_df[train_df.SELLER_DEPARTMENT == 0].shape[0],\n",
    "            train_df[train_df.SELLER_DEPARTMENT == 96].shape[0],\n",
    "            train_df[train_df.SELLER_DEPARTMENT == 97].shape[0],\n",
    "            train_df[train_df.SELLER_DEPARTMENT == 98].shape[0]\n",
    "        )\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "42 318 valeurs à -1 parce qu'a l'étranger, ça me semble impossible à gérer de façon smart\n",
    "\n",
    "On remplace tout les autres à zéro? Oui? Très bien\n",
    "''' \n",
    "\n",
    "train_df.SELLER_DEPARTMENT = train_df.SELLER_DEPARTMENT.apply(departement_filler)\n",
    "test_df.SELLER_DEPARTMENT = test_df.SELLER_DEPARTMENT.apply(departement_filler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PRODUCT_FAMILY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"On a {} valeurs à WHITE\".format(train_df[train_df.PRODUCT_FAMILY == 'WHITE'].shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un peu complexe là, on sait que WHITE contient une majorité de `PRODUCT_TYPE` \"BRICOLAGE\", donc bon, faudrait refaire un mapping dans la classe, mais j'ai la flemme"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bilan des corrections d'erreurs et de données manquantes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ben on est bien là! On a corrigé nos trucs, et tout le monde est content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.info()\n",
    "print('_'*40)\n",
    "test_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traitement des variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Variables Mixtes**\n",
    "\n",
    "C'est à dire qu'elles contiennes en fait de l'alphanumérique, et des structures à l'interieur\n",
    "\n",
    "1. `SHIPPING_PRICE`         - Format Nombre </> Nombre\n",
    "2. `WARRANTIES_PRICE`         - Format Nombre </> Nombre\n",
    "3. `PURCHASE_COUNT`         - Format Nombre </> Nombre\n",
    "4. `BUYING_DATE`         - Format Nombre/2017\n",
    "5. `SELLER_SCORE_COUNT`         - Format Nombre </> Nombre\n",
    "6. `ITEM_PRICE`         - Format Nombre </> Nombre"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bon, en y regardant de plus près:\n",
    "\n",
    "On ne peut pas tirer de ces variables catégorielles des variables continues.\n",
    "\n",
    "La seule information que nous avons c'est l'ordre de grandeur. Alors transformons les en variables __ordinales__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SHIPPING_PRICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemple avant\n",
    "print(train_df.SHIPPING_PRICE.sort_values().unique())\n",
    "print(test_df.SHIPPING_PRICE.sort_values().unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapper= {'0':0, '<1':1, '1<5':2, '5<10':3, '10<20':4, '>20':5}\n",
    "\n",
    "train_df.SHIPPING_PRICE = train_df.SHIPPING_PRICE.map(mapper)\n",
    "test_df.SHIPPING_PRICE = test_df.SHIPPING_PRICE.map(mapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Et maintenant après transformation\n",
    "print(train_df.SHIPPING_PRICE.sort_values().unique())\n",
    "print(test_df.SHIPPING_PRICE.sort_values().unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WARRANTIES_PRICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_df.WARRANTIES_PRICE.sort_values().unique())\n",
    "print(test_df.WARRANTIES_PRICE.sort_values().unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapper={'0':0, '<5':1, '5<20':2, '20<50':3, '50<100':4,  '100<500':5}\n",
    "\n",
    "train_df.WARRANTIES_PRICE = train_df.WARRANTIES_PRICE.map(mapper)\n",
    "test_df.WARRANTIES_PRICE = test_df.WARRANTIES_PRICE.map(mapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_df.WARRANTIES_PRICE.sort_values().unique())\n",
    "print(test_df.WARRANTIES_PRICE.sort_values().unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PURCHASE_COUNT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_df.PURCHASE_COUNT.sort_values().unique())\n",
    "print(test_df.PURCHASE_COUNT.sort_values().unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapper={'<5':0, '5<20':1, '20<50':2, '50<100':3, '100<500':4,  '>500':5}\n",
    "\n",
    "train_df.PURCHASE_COUNT = train_df.PURCHASE_COUNT.map(mapper)\n",
    "test_df.PURCHASE_COUNT = test_df.PURCHASE_COUNT.map(mapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_df.PURCHASE_COUNT.sort_values().unique())\n",
    "print(test_df.PURCHASE_COUNT.sort_values().unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BUYING_DATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_df.BUYING_DATE.sort_values().unique())\n",
    "print(test_df.BUYING_DATE.sort_values().unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toto= train_df.BUYING_DATE.str.split('/', 1, expand=True)\n",
    "train_df.BUYING_DATE = toto[0]\n",
    "train_df.BUYING_DATE = train_df.BUYING_DATE.apply(pd.to_numeric, errors='ignore')\n",
    "\n",
    "toto= test_df.BUYING_DATE.str.split('/', 1, expand=True)\n",
    "test_df.BUYING_DATE = toto[0]\n",
    "test_df.BUYING_DATE = test_df.BUYING_DATE.apply(pd.to_numeric, errors='ignore')\n",
    "\n",
    "del toto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_df.BUYING_DATE.sort_values().unique())\n",
    "print(test_df.BUYING_DATE.sort_values().unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SELLER_SCORE_COUNT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_df.SELLER_SCORE_COUNT.sort_values().unique())\n",
    "print(test_df.SELLER_SCORE_COUNT.sort_values().unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapper={'<100':0, '100<1000':1, '1000<10000':2, '10000<100000':3, '100000<1000000':4}\n",
    "\n",
    "train_df.SELLER_SCORE_COUNT = train_df.SELLER_SCORE_COUNT.map(mapper)\n",
    "test_df.SELLER_SCORE_COUNT = test_df.SELLER_SCORE_COUNT.map(mapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_df.SELLER_SCORE_COUNT.sort_values().unique())\n",
    "print(test_df.SELLER_SCORE_COUNT.sort_values().unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ITEM_PRICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_df.ITEM_PRICE.sort_values().unique())\n",
    "print(test_df.ITEM_PRICE.sort_values().unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapper={'<10':0, '10<20':1, '20<50':2, '50<100':3, '100<500':4,  '500<1000':5, '1000<5000':6, '>5000':6} #Il existe qu'un >5000, et il n'existe pas dans le jeu de test\n",
    "\n",
    "train_df.ITEM_PRICE = train_df.ITEM_PRICE.map(mapper)\n",
    "test_df.ITEM_PRICE = test_df.ITEM_PRICE.map(mapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_df.ITEM_PRICE.sort_values().unique())\n",
    "print(test_df.ITEM_PRICE.sort_values().unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PRICECLUB_STATUS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_df.PRICECLUB_STATUS.sort_values().unique())\n",
    "print(test_df.PRICECLUB_STATUS.sort_values().unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le classement des types de comptes est inscrit ici : https://www.priceminister.com/evt/rakuten-superpoints\n",
    "\n",
    "En faisant ça, je veux garder l'information \"classement\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapper={'UNSUBSCRIBED':0, 'REGULAR':1, 'SILVER':2, 'GOLD':3, 'PLATINUM':4}\n",
    "\n",
    "train_df.PRICECLUB_STATUS = train_df.PRICECLUB_STATUS.map(mapper)\n",
    "test_df.PRICECLUB_STATUS = test_df.PRICECLUB_STATUS.map(mapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_df.PRICECLUB_STATUS.sort_values().unique())\n",
    "print(test_df.PRICECLUB_STATUS.sort_values().unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop Duplicates\n",
    "\n",
    "\n",
    "Finding duplicates in your data is important because you end up:\n",
    "\n",
    "1. Spending more computational time to process duplicates, which slows your algorithms down.\n",
    "\n",
    "2. Obtaining false results because duplicates implicitly overweight the results. \n",
    "\n",
    "Because some entries appear more than once, the algorithm considers these entries more important.\n",
    "\n",
    "Ok, mais ici, on a aucun duplicate dans la classe majoritaire, donc je préfère ne rien droper finalement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.drop_duplicates(inplace=False).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mais si on enlève les duplicates on perd 3149 lignes... Donc c'est mort de mon point de vue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bilan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ben on est bien là! On a transformé tout ça en variables __ordinales__, et tout le monde est content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.info()\n",
    "print('_'*40)\n",
    "test_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actuellement, il nous reste quatre variables catégorielles:\n",
    "1. `SHIPPING_MODE`\n",
    "2. `SELLER_COUNTRY`\n",
    "3. `PRODUCT_TYPE`\n",
    "4. `PRODUCT_FAMILY`\n",
    "\n",
    "Il n'y a pas de classement à faire entre ces catégories, on verra ça dans le Notebook 2 Feature Engineering.\n",
    "\n",
    "Mais quel teasing!\n",
    "\n",
    "Et on oublie pas d'écrire les résultats de nos transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename_train = DATA_PROCESSED+\"/train_notebook_1.csv\"\n",
    "filename_test = DATA_PROCESSED+\"/test_notebook_1.csv\"\n",
    "\n",
    "try:\n",
    "    os.remove(filename_train)\n",
    "    os.remove(filename_test)\n",
    "except:\n",
    "    pass;\n",
    "    \n",
    "train_df.to_csv(filename_train, index=False, sep=\";\")\n",
    "test_df.to_csv(filename_test, index=False, sep=\";\")"
   ]
  }
 ],
 "metadata": {
  "_change_revision": 0,
  "_is_fork": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
