{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aujourd'hui on roule sur les mecs de l'ENS\n",
    "\n",
    "\n",
    "https://challengedata.ens.fr/en/challenge/39/prediction_of_transaction_claims_status.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports des librairies de bases\n",
    "\n",
    "On ajoutera celles qui manquent au fur et à mesure de nos besoins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To support both python 2 and python 3\n",
    "from __future__ import division, print_function, unicode_literals\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os, gc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Définition de la seed pour le random\n",
    "\n",
    "Très important pour qu'on voit les mêmes choses entre nos deux ordis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 42;\n",
    "np.random.seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Définition des paramètres pour Matplot\n",
    "\n",
    "Rien de bien intéréssant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 12\n",
    "plt.rcParams['ytick.labelsize'] = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set des variables globales\n",
    "\n",
    "Attention, je n'utilise les variables globales pour la gestion des fichiers. Sinon, c'est mort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where to save the figures\n",
    "PROJECT_ROOT_DIR = \"..\"\n",
    "DATA_PROCESSED = os.path.join(PROJECT_ROOT_DIR, \"data_processed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fonction pour load les libraires\n",
    "\n",
    "En vrai, on a juste besoin de pd.read_csv, mais c'était pour faire joli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file,data_path=DATA_PROCESSED, sep=';'):\n",
    "    csv_path = os.path.join(data_path, file)\n",
    "    return pd.read_csv(csv_path, sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# On load les jeux de données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TX_data = load_data(file = \"train.csv\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TX_data.drop(['CARD_PAYMENT','COUPON_PAYMENT','RSP_PAYMENT','WALLET_PAYMENT'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "TX_data.info() # 42 colonnes, c'est un nombre qui fait plaisir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vérification des corrélations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_map=TX_data.select_dtypes(exclude=\"object\").astype(float).corr(method='spearman')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colormap = plt.cm.BrBG\n",
    "plt.figure(figsize=(12,12))\n",
    "plt.title('Pearson Correlation of Features', y=1.05, size=15)\n",
    "sns.heatmap(corr_map,\n",
    "            linewidths=0.1,\n",
    "            vmax=1.0, \n",
    "            square=True, \n",
    "            cmap=colormap, \n",
    "            linecolor='white',\n",
    "            annot=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_set, test_set = train_test_split(TX_data, \n",
    "                                       test_size=0.3, \n",
    "                                       random_state=RANDOM_SEED, \n",
    "                                       stratify=TX_data[\"CLAIM_TYPE\"]\n",
    "                                      )\n",
    "del TX_data;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jointure entre les X et Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def datapreprocess(data):\n",
    "    data=data.apply(pd.to_numeric, errors='ignore')\n",
    "    \n",
    "    # Y and X\n",
    "    try :\n",
    "        Y=data[\"CLAIM_TYPE\"]\n",
    "        X=data.drop(\"CLAIM_TYPE\", axis=1,inplace=False)\n",
    "    except:\n",
    "        Y=0\n",
    "        X=data\n",
    "    # Exclude Objets\n",
    "    X=X.select_dtypes(exclude=['object']) # j'exclude les variables catégorielles que j'ai oublié\n",
    "    \n",
    "    # Work on fare\n",
    "    from sklearn.preprocessing import Imputer\n",
    "    imp = Imputer(missing_values='NaN',strategy='median', axis=1)\n",
    "    X=pd.DataFrame(imp.fit_transform(X),columns=X.columns.values)\n",
    " \n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, Y_train = datapreprocess(train_set)\n",
    "X_test, Y_test = datapreprocess(test_set)\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiclass_roc_auc_score(truth, pred):\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    from sklearn.preprocessing import LabelBinarizer\n",
    "    lb = LabelBinarizer()\n",
    "    lb.fit(truth)\n",
    "    return roc_auc_score(lb.transform(truth), lb.transform(pred), average=\"weighted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "def plot_confusion_matrix(matrix):\n",
    "    \"\"\"If you prefer color and a colorbar\"\"\"\n",
    "    fig = plt.figure(figsize=(8,8))\n",
    "    ax = fig.add_subplot(111)\n",
    "    cax = ax.matshow(matrix)\n",
    "    fig.colorbar(cax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils.class_weight import compute_sample_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_weight_arr = compute_sample_weight(class_weight='balanced', y=Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Core XGBoost Library VS scikit-learn API\n",
    "\n",
    "Models can be trained in two different ways:\n",
    "\n",
    "1. Directly using the core library – this is closer to the implementation of the caret-package in R\n",
    "2. Using the scikit-learn API – this means that the models are implemented in a way that lets the scikit package recognize it as one of it’s own models.\n",
    "\n",
    "Nous, on va travailler avec l'API de Sklearn, c'est pas optimisé mais plus simple. De toute façon, j'arrive pas à utiliser le Core, a cause des DMatrix qui veulent que des numerics en entrées\n",
    "\n",
    "Doc des paramètres: https://github.com/dmlc/xgboost/blob/master/doc/parameter.md\n",
    "\n",
    "Doc sur le tunning : https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  XGBoost solo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_XGB={\n",
    "# General Parameters -  the overall functioning\n",
    "    'booster':'gbtree',\n",
    "    'silent':0,\n",
    "    #'nthread':4, # Je le commente, puisque il détecte automatiquement le nombre de cores qu'il peut utiliser.\n",
    "    'n_estimators' : 1000,\n",
    "    \n",
    "# Booster Parameters - the individual booster (tree/regression) at each step\n",
    "    'learning_rate' : 0.1,\n",
    "    'min_child_weight' : 1, #A smaller value is chosen because it is a highly imbalanced class problem and leaf nodes can have smaller size groups.\n",
    "    'max_depth' : 3,\n",
    "    #'max_leaf_nodes':None, #If this is defined, GBM will ignore max_depth.\n",
    "    'gamma' : 0.3,\n",
    "    'max_delta_step':4, #it might help in logistic regression when class is extremely imbalanced/ 1-10 might help control the update\n",
    "    'subsample' : 0.55,\n",
    "    'colsample_bytree' : 0.85,\n",
    "    'colsample_bylevel':1, #default\n",
    "    'reg_lambda' : 1, #default\n",
    "    'reg_alpha':0,\n",
    "    'scale_pos_weight' : sample_weight_arr,\n",
    "\n",
    "# Learning Task Parameters -  the optimization performed\n",
    "    'objective' : 'multi:softmax', # you also need to set an additional num_class (number of classes)\n",
    "    'num_class' : len(Y_train.unique()),\n",
    "    'eval_metric':\"auc\",\n",
    "    'seed' : RANDOM_SEED,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_clf = XGBClassifier(**params_XGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_clf.fit(\n",
    "    X=X_train, \n",
    "    y=Y_train, \n",
    "    sample_weight=sample_weight_arr, \n",
    "    eval_set=None, \n",
    "    eval_metric='auc', \n",
    "    early_stopping_rounds=None, \n",
    "    verbose=True, \n",
    "    xgb_model=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_xgb_train = xgb_clf.predict(X_train)\n",
    "y_pred_xgb = xgb_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mAUC = multiclass_roc_auc_score(Y_train, y_pred_xgb_train)\n",
    "test_mAUC = multiclass_roc_auc_score(Y_test, y_pred_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Performance sur le train : {}\".format(train_mAUC))\n",
    "print(\"Performance sur le test : {}\".format(test_mAUC))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performance sur le train : 0.6589482571844301\n",
    "\n",
    "Performance sur le test : 0.6180906043249655\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_mx = confusion_matrix(Y_test, y_pred_xgb)\n",
    "row_sums = conf_mx.sum(axis=1, keepdims=True)\n",
    "norm_conf_mx = conf_mx / row_sums\n",
    "plot_confusion_matrix(norm_conf_mx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### C'est un beau score pour le XBoost\n",
    "\n",
    "Cependant, j'ai optimisé pour la mauvaise métrique, et j'ai toujours pas fait le Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(xgb_clf.feature_importances_, index=X_train.columns, columns=[\"Feature\"]).sort_values(by=\"Feature\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sortie en erreur les copines!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost Grid Search Iterate\n",
    "\n",
    "Comme mon PC est pourri, je vais chercher les paramètres de façon iterative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Paramètres pour le XGB qui ne changent pas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_XGB={\n",
    "# General Parameters -  the overall functioning\n",
    "    'booster':'gbtree',\n",
    "    'silent':0,\n",
    "    #'nthread':4, # Je vais le commenter, puisque il détecte automatiquement le nombre de cores qu'il peut utiliser.\n",
    "    'n_estimators' : 100,\n",
    "    \n",
    "# Booster Parameters - the individual booster (tree/regression) at each step\n",
    "    'learning_rate' : 0.1,\n",
    "    'colsample_bylevel':1, #default\n",
    "    'reg_lambda' : 1, #default\n",
    "    'scale_pos_weight' : sample_weight_arr,\n",
    "\n",
    "# Learning Task Parameters -  the optimization performed\n",
    "    'objective' : 'multi:softmax',\n",
    "    'num_class' : len(Y_train.unique()),\n",
    "    'eval_metric':\"auc\",\n",
    "    'seed' : RANDOM_SEED,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Paramètres pour la méthode `fit` de XGB qui ne changent pas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_params_xgb_cv={\n",
    "    'sample_weight': sample_weight_arr, \n",
    "    'eval_set' : None, \n",
    "    'eval_metric' : 'auc', \n",
    "    'early_stopping_rounds' : None, \n",
    "    'verbose':True, \n",
    "    'xgb_model':None\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Optim 1 : max_depth, min_child_weight et max_delta_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paramètres pour le GridSearch\n",
    "params_XGB_CV = {\n",
    "    'max_depth':range(2,5),\n",
    "    'min_child_weight':range(1,6),\n",
    "    'max_delta_step':list(range(1,5)),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_gs_cv = GridSearchCV(XGBClassifier(**params_XGB), \n",
    "                              params_XGB_CV,\n",
    "                              n_jobs=-1,\n",
    "                              verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_gs_cv.fit(\n",
    "    X = X_train, \n",
    "    y=Y_train, \n",
    "    groups=None, \n",
    "    **fit_params_xgb_cv\n",
    ")\n",
    "print(xgb_gs_cv.best_estimator_)\n",
    "print(\"ROC score : {}\".format(multiclass_roc_auc_score(Y_test, xgb_gs_cv.predict(X_test))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Donc les paramètres optimaux sont:\n",
    "1. `max_depth` :\n",
    "2. `min_child_weight` :\n",
    "3. `max_delta_step` :\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Optim 2 : gamma, et subsample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paramètres pour le GridSearch\n",
    "params_XGB_CV = {\n",
    "    'gamma':[i/10.0 for i in range(0,5)],\n",
    "    'subsample':[i/10.0 for i in range(6,10)],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_gs_cv = GridSearchCV(XGBClassifier(**params_XGB), \n",
    "                              params_XGB_CV,\n",
    "                              n_jobs=-1,\n",
    "                              verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_gs_cv.fit(\n",
    "    X = X_train, \n",
    "    y=Y_train, \n",
    "    groups=None, \n",
    "    **fit_params_xgb_cv\n",
    ")\n",
    "print(xgb_gs_cv.best_estimator_)\n",
    "print(\"ROC score : {}\".format(multiclass_roc_auc_score(Y_test, xgb_gs_cv.predict(X_test))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Donc les paramètres optimaux sont:\n",
    "1. `gamma` :\n",
    "2. `subsample` :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Optim 3 : colsample_bytree, et reg_alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paramètres pour le GridSearch\n",
    "params_XGB_CV = {\n",
    "    'colsample_bytree':[i/10.0 for i in range(6,10)],\n",
    "    'reg_alpha':[0, 0.001, 0.005, 0.01, 0.05]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_gs_cv = GridSearchCV(XGBClassifier(**params_XGB), \n",
    "                              params_XGB_CV,\n",
    "                              n_jobs=-1,\n",
    "                              verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_gs_cv.fit(\n",
    "    X = X_train, \n",
    "    y=Y_train, \n",
    "    groups=None, \n",
    "    **fit_params_xgb_cv\n",
    ")\n",
    "print(xgb_gs_cv.best_estimator_)\n",
    "print(\"ROC score : {}\".format(multiclass_roc_auc_score(Y_test, xgb_gs_cv.predict(X_test))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Donc les paramètres optimaux sont:\n",
    "1. `colsample_bytree` :\n",
    "2. `reg_alpha` :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_mx = confusion_matrix(Y_test, y_pred_xgb_cv)\n",
    "row_sums = conf_mx.sum(axis=1, keepdims=True)\n",
    "norm_conf_mx = conf_mx / row_sums\n",
    "plot_confusion_matrix(norm_conf_mx)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
