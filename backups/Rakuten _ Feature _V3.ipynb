{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aujourd'hui on roule sur les mecs de l'ENS\n",
    "\n",
    "\n",
    "https://challengedata.ens.fr/en/challenge/39/prediction_of_transaction_claims_status.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports des librairies de bases\n",
    "\n",
    "On ajoutera celles qui manquent au fur et à mesure de nos besoins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To support both python 2 and python 3\n",
    "from __future__ import division, print_function, unicode_literals\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os, gc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Définition de la seed pour le random\n",
    "\n",
    "Très important pour qu'on voit les mêmes choses entre nos deux ordis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 42;\n",
    "np.random.seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Définition des paramètres pour Matplot\n",
    "\n",
    "Rien de bien intéréssant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 12\n",
    "plt.rcParams['ytick.labelsize'] = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set des variables globales\n",
    "\n",
    "Attention, je n'utilise les variables globales pour la gestion des fichiers. Sinon, c'est mort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where to save the figures\n",
    "PROJECT_ROOT_DIR = \".\"\n",
    "DATA_PATH = os.path.join(PROJECT_ROOT_DIR, \"data\")\n",
    "DATA_PROCESSED = os.path.join(PROJECT_ROOT_DIR, \"data_processed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fonction pour load les libraires\n",
    "\n",
    "En vrai, on a juste besoin de pd.read_csv, mais c'était pour faire joli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file,data_path=DATA_PATH, sep=','):\n",
    "    csv_path = os.path.join(data_path, file)\n",
    "    return pd.read_csv(csv_path, sep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# On load les jeux de données\n",
    "\n",
    "Seulement le train (input_train) pour le moment. Et le \"Y\" des lignes dans le fichier avec le long nom.\n",
    "Je ne sais pas pourquoi ils ont tout divisé, mais peût être pour filter les mecs qui ne savent pas fire des jointures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TX_data = load_data(file = \"input_train.csv\");\n",
    "STA_data = load_data(file = \"challenge_output_data_training_file_prediction_of_transaction_claims_status.csv\", sep=';');\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jointure entre les X et Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge de TX_data et STA_data en utilisant les index comme cléf primaire\n",
    "TX_data = pd.merge(TX_data, STA_data, left_index=True, right_index=True)\n",
    "\n",
    "# On drop les ID qui sont crée dans le processus\n",
    "TX_data.drop([\"ID_y\",\"ID_x\"],inplace=True,axis=1)\n",
    "\n",
    "# On applique tout ce qui peut l'être en numérique\n",
    "TX_data=TX_data.apply(pd.to_numeric, errors='ignore')\n",
    "\n",
    "del STA_data #Je supprime toujours les variables inutiles pour liberer ma RAM (qui est très faible)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fonction de mise en forme (attention, sportive)\n",
    "\n",
    "Dans la BDD, j'ai vu pas mal de truc du genre \"20<50\", inutilisables. Donc, j'ai voulu splité pour faire une colonne avec le \"20\" (MIN), et une autre avec le \"50\" (MAX).\n",
    "Donc j'ai fait cette fonction qui prend en entrée la BDD, une liste de noms de colones à traiter, et modifie la base de donnée directement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rakuten import Splitter\n",
    "\n",
    "list_col_split=[\"WARRANTIES_PRICE\",\n",
    "                'SELLER_SCORE_COUNT',\n",
    "                'ITEM_PRICE',\n",
    "                'PURCHASE_COUNT',\n",
    "                'SHIPPING_PRICE']\n",
    "\n",
    "# La classe \"Splitter\" prend en init (le constructeur) la liste des colonnes qu'elle va traiter\n",
    "splitter_cell = Splitter(list_col=list_col_split)\n",
    "\n",
    "# On applique la transformation sur le jeu de données\n",
    "TX_data = splitter_cell.transform(TX_data)\n",
    "\n",
    "del list_col_split, splitter_cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rakuten import DateSplitter\n",
    "\n",
    "# La classe \"DateSplitter\" prend en init (le constructeur) la liste des colonnes qu'elle va traiter\n",
    "splitter_date = DateSplitter('BUYING_DATE')\n",
    "\n",
    "# On applique la transformation sur le jeu de données\n",
    "TX_data = splitter_date.transform(TX_data)\n",
    "\n",
    "del splitter_date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Là, faut être créatif, et aider l'algorithme à voir des choses, et se focaliser sur d'autres.\n",
    "\n",
    "Je vais rarement drop des colonnes, car à ce moment là, je ne sais pas ce qui est utile ou pas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A une garantie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rakuten import BoolMapper\n",
    "\n",
    "# La classe \"BoolMapper\" prend en init (le constructeur) la liste des colonnes qu'elle va traiter\n",
    "boolMap = BoolMapper(col='WARRANTIES_FLG')\n",
    "\n",
    "# On applique la transformation sur le jeu de données\n",
    "TX_data = boolMap.transform(TX_data)\n",
    "\n",
    "del boolMap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Taux de couverture de la garantie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rakuten import WarrantCov\n",
    "\n",
    "# Ne prend rien en entrée, elle sait déjà quelle colonne elle va traiter (parce que c'est mal codé)\n",
    "couverture = WarrantCov()\n",
    "\n",
    "# On applique\n",
    "TX_data = couverture.transform(TX_data)\n",
    "\n",
    "del couverture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shipping Method\n",
    "\n",
    "Regroupement des méthodes de shipping rares sous la même catégorie.\n",
    "\n",
    "Objectif = Diminuer la complexité de la variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rakuten import Dummyzator\n",
    "\n",
    "list_rare_delivery_type = ['EXPRESS_DELIVERY',\n",
    "                          'SO_RECOMMANDE',\n",
    "                          'MONDIAL_RELAY',\n",
    "                          'MONDIAL_RELAY_PREPAYE',\n",
    "                          'SO_POINT_RELAIS',\n",
    "                          'CHRONOPOST',\n",
    "                          'PICKUP',\n",
    "                          'Kiala']\n",
    "\n",
    "# La classe \"Dummyzator\" prend en init (le constructeur):\n",
    "# 1. La cible (colonne) pour remplacer les valeurs sur \"target\"\n",
    "# 2. la liste des valeurs à remplacer sur \"list_col\"\n",
    "# 3. La valeur à changer sur \"new_value\"\n",
    "\n",
    "# Elle renvoit la même base de données, mais avec la cible dummyfiée (une colonne binaire par valeur de la cible)\n",
    "\n",
    "dummyShip = Dummyzator(target = \"SHIPPING_MODE\", \n",
    "                     list_col = list_rare_delivery_type, \n",
    "                     new_value = \"RARE_TYPE\")\n",
    "\n",
    "TX_data=dummyShip.transform(TX_data)\n",
    "\n",
    "del list_rare_delivery_type, dummyShip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Customer Premium\n",
    "\n",
    "Regroupement des types de comptes payant sous la même catégorie.\n",
    "\n",
    "Objectif = Diminuer la complexité de la variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_rare_client_type= ['PLATINUM', \n",
    "                          'SILVER', \n",
    "                          'GOLD']\n",
    "\n",
    "dummyPrice=Dummyzator(target = \"PRICECLUB_STATUS\",\n",
    "                      list_col = list_rare_client_type,\n",
    "                      new_value = \"PREMIUM\")\n",
    "\n",
    "TX_data=dummyPrice.transform(TX_data)\n",
    "\n",
    "del list_rare_client_type, dummyPrice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trajets longs\n",
    "\n",
    "Flag pour signaler un envoi international\n",
    "\n",
    "Objectif = souligner les courts trajets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(row):\n",
    "    long_haul_list = [\"CHINA\",\"HONG KONG\", \"UNITED STATES\"]\n",
    "    return 1 if row['SELLER_COUNTRY'] in long_haul_list else 0;\n",
    "\n",
    "TX_data['LONG_HAUL'] = TX_data.apply(f, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stats par Pays\n",
    "\n",
    "To do : Détecter les pays à problèmes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rakuten import Stat_Adder\n",
    "\n",
    "sell_country_stat = Stat_Adder(target = \"SELLER_COUNTRY\")\n",
    "\n",
    "sell_country_stat.fit(TX_data)\n",
    "\n",
    "TX_data = sell_country_stat.transform(TX_data)\n",
    "\n",
    "del sell_country_stat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Type de produit\n",
    "\n",
    "To do : Détecter les produits fragiles\n",
    "\n",
    "Objectif = souligner cet effet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_stat = Stat_Adder(target = \"PRODUCT_TYPE\")\n",
    "\n",
    "product_stat.fit(TX_data)\n",
    "\n",
    "TX_data = product_stat.transform(TX_data)\n",
    "\n",
    "del product_stat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Création d'un ID par client \n",
    "\n",
    "Objectif = créer une base de données client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rakuten import ID_Stat_Adder\n",
    "\n",
    "id_Concat_buyer = ['REGISTRATION_DATE',\"BUYER_DEPARTMENT\",\"BUYER_BIRTHDAY_DATE\"]\n",
    "\n",
    "buyerID = ID_Stat_Adder(ID = 'BUYER_ID', \n",
    "                        concatenator = id_Concat_buyer)\n",
    "\n",
    "buyerID.fit(TX_data)\n",
    "\n",
    "TX_data = buyerID.transform(TX_data)\n",
    "\n",
    "del id_Concat_buyer, buyerID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Création d'un ID par vendeur\n",
    "\n",
    "Objectif = créer une base de données vendeur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_Concat_seller = ['SELLER_COUNTRY',\"SELLER_DEPARTMENT\",\"SELLER_SCORE_AVERAGE\"]\n",
    "\n",
    "sellID = ID_Stat_Adder(ID = 'SELLER_ID', \n",
    "                        concatenator = id_Concat_seller)\n",
    "\n",
    "sellID.fit(TX_data)\n",
    "\n",
    "TX_data = sellID.transform(TX_data)\n",
    "\n",
    "del id_Concat_seller, sellID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fusion des bases de données\n",
    "\n",
    "Objectif = Yolo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_to_drop=[\"BUYER_ID\",\"SELLER_ID\", \"0_x\", \"0_y\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TX_data.drop(list_to_drop,inplace=True,axis=1)\n",
    "\n",
    "del list_to_drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# On essaie d'ajouter des composantes de Neighbors, PCA, LDA, etc.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def datapreprocess_LDA(data):\n",
    "    data=data.apply(pd.to_numeric, errors='ignore')\n",
    "    \n",
    "    # Y and X\n",
    "    try: \n",
    "        Y=data[\"CLAIM_TYPE\"]\n",
    "        X=data.drop(\"CLAIM_TYPE\", axis=1,inplace=False)\n",
    "    except:\n",
    "        Y=0\n",
    "        X=data;\n",
    "        pass;\n",
    "    # Exclude Objets\n",
    "    X=X.select_dtypes(exclude=['object'])\n",
    "    \n",
    "    # Work on fare\n",
    "    from sklearn.preprocessing import Imputer\n",
    "    imp = Imputer(missing_values='NaN',strategy='median', axis=1)\n",
    "    X=pd.DataFrame(imp.fit_transform(X),columns=X.columns.values)\n",
    " \n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,Y=datapreprocess_LDA(TX_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2)\n",
    "X_r = pd.DataFrame(pca.fit(X).transform(X), columns=['PCA_1', 'PCA_2'])\n",
    "TX_data['PCA_1']=X_r['PCA_1']\n",
    "TX_data['PCA_2']=X_r['PCA_2']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "del datapreprocess_LDA, pca, X_r, lda, X_r2\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Et on écrit la BDD sur un joli CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = DATA_PROCESSED+\"/working_data.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.remove(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TX_data.to_csv(path_or_buf=filename, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A partir de là, je fais juste un test bidon pour voir si les features apportent quelque chose\n",
    "\n",
    "## Les vrais modèles sont dans le notebook \"Models\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On coupe\n",
    "\n",
    "Objectif = Créer un jeu de données train et test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_set, test_set = train_test_split(TX_data, \n",
    "                                       test_size=0.3, \n",
    "                                       random_state=RANDOM_SEED, \n",
    "                                       stratify=TX_data[\"CLAIM_TYPE\"]\n",
    "                                      )\n",
    "del TX_data;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess\n",
    "\n",
    "Objectif = Rendre joli tout ça"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def datapreprocess(data):\n",
    "    data=data.apply(pd.to_numeric, errors='ignore')\n",
    "    \n",
    "    # Y and X\n",
    "    try :\n",
    "        Y=data[\"CLAIM_TYPE\"]\n",
    "        X=data.drop(\"CLAIM_TYPE\", axis=1,inplace=False)\n",
    "    except:\n",
    "        Y=0\n",
    "        X=data\n",
    "    # Exclude Objets\n",
    "    X=X.select_dtypes(exclude=['object'])\n",
    "    \n",
    "    # Work on fare\n",
    "    from sklearn.preprocessing import Imputer\n",
    "    imp = Imputer(missing_values='NaN',strategy='median', axis=1)\n",
    "    X=pd.DataFrame(imp.fit_transform(X),columns=X.columns.values)\n",
    " \n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,Y_train=datapreprocess(train_set)\n",
    "X_test, Y_test=datapreprocess(test_set)\n",
    "\n",
    "del train_set, test_set;\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metriques\n",
    "\n",
    "D'abord, notre métrique à nous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rakuten import multiclass_roc_auc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notre Random Forest tout simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnd_clf = RandomForestClassifier(n_jobs=-1, \n",
    "                                 random_state=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnd_clf.fit(X_train, Y_train);\n",
    "y_pred_rf = rnd_clf.predict(X_test)\n",
    "multiclass_roc_auc_score(Y_test, y_pred_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.multiclass import OneVsOneClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "ovo_clf = OneVsOneClassifier(SGDClassifier(max_iter=5, random_state=42))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ovo_clf.fit(X_train, Y_train)\n",
    "y_pred_ovo=ovo_clf.predict(X_test)\n",
    "multiclass_roc_auc_score(Y_test, y_pred_ovo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(matrix):\n",
    "    \"\"\"If you prefer color and a colorbar\"\"\"\n",
    "    fig = plt.figure(figsize=(8,8))\n",
    "    ax = fig.add_subplot(111)\n",
    "    cax = ax.matshow(matrix)\n",
    "    fig.colorbar(cax)\n",
    "    \n",
    "conf_mx = confusion_matrix(Y_test, y_pred_ovo)\n",
    "plot_confusion_matrix(conf_mx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "row_sums = conf_mx.sum(axis=1, keepdims=True)\n",
    "norm_conf_mx = conf_mx / row_sums\n",
    "plot_confusion_matrix(norm_conf_mx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from rakuten import rakuten_ROC_CV\n",
    "\n",
    "rakuten_ROC_CV(rnd_clf, X_train, Y_train, _n_splits=10, _random_state=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(n_jobs=-1, \n",
    "                             random_state=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sfm_clf = SelectFromModel(clf)\n",
    "del clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_clf=GradientBoostingClassifier(random_state=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import Normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = Pipeline([\n",
    "    #('scaler', StandardScaler()), \n",
    "    #('norm', Normalizer()), \n",
    "    #('threshold', sel_vt), \n",
    "    #('SelectKBest', Chi_select), \n",
    "    #('reduce_dim', PCA()),\n",
    "    #('feature_union', combined),\n",
    "    #('feature_selection', sfm_clf),\n",
    "    ('classification', gss)\n",
    "])\n",
    "\n",
    "\n",
    "clf.fit(X_train, Y_train);\n",
    "y_pred_rf = clf.predict(X_test)\n",
    "multiclass_roc_auc_score(Y_test, y_pred_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_mx = confusion_matrix(Y_test, y_pred_rf)\n",
    "row_sums = conf_mx.sum(axis=1, keepdims=True)\n",
    "norm_conf_mx = conf_mx / row_sums\n",
    "plot_confusion_matrix(norm_conf_mx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = XGBClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_xgb = model.predict(X_test)\n",
    "multiclass_roc_auc_score(Y_test, y_pred_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_mx = confusion_matrix(Y_test, y_pred_xgb)\n",
    "row_sums = conf_mx.sum(axis=1, keepdims=True)\n",
    "norm_conf_mx = conf_mx / row_sums\n",
    "plot_confusion_matrix(norm_conf_mx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(Y_train.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "target_names = Y_train.unique()\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "X_r = pca.fit(X_train).transform(X_train)\n",
    "\n",
    "# Percentage of variance explained for each components\n",
    "print('explained variance ratio (first two components): %s'\n",
    "      % str(pca.explained_variance_ratio_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toto=Y_train.reset_index()\n",
    "toto.drop(\"index\", inplace=True, axis=1)\n",
    "X_r=pd.DataFrame(X_r, columns=[\"one\",\"two\"])\n",
    "X_r['result']=toto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "g = sns.FacetGrid(X_r, hue=\"result\", size=7)\n",
    "g.map(plt.scatter, \"one\", \"two\", s=50, alpha=.7, linewidth=.5, edgecolor=\"white\")\n",
    "g.add_legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.pairplot(X_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
